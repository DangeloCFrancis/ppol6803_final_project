---
title: "Predicting Electoral Outcomes using Demographic Characterisitcs"
author: "D'Angelo Francis; Su Yeon Seo; Yuxiang 'Nathan' Su"
format: html
---


## Purpose

Our research project seeks to identify the key characteristics to identify and predict how a voter votes. 
 This project utilizes a Classification and Regression Tree model, and 'glm' (logistic and probit regression) models to predict the probability that a survey respondent voted 'conservatively' or 'liberally'[^1]. The primary data for analysis and modeling will be survey data from the [American National Election Studies](https://electionstudies.org/), a collaboration between Duke University,the University of Michigan,the University of Texas at Austin (UT Austin), Stanford University, and the National Science Foundation (NSF).  

For our model, we plan on using questions that the ANES asks about a candidate that voter voted for (*e.g., "Which candidate did you vote for in...?"*) as the outcome variable our model will predict. The goal of our models are to precisely predict which a voter voted for using the responses from survey questions asking about policies, such as transgender rights, gun control, reproductive rights, etc. We will incorporate a variable importance analysis to deduce which survey topics and demographic characteristics are the best indicators of voters' candidate preference.

## Setting up our models 

```{r}
#| label: setup 

set.seed(41224) # Euro style date for December 4,2024

# core packages

library(tidyverse)
library(tidymodels) # use case weights 
library(readxl)
library(baguette)
library(vip)

# visualization packages

library(patchwork) # easy combine plots
library(tigris) # direct download of shape files from US Census 
library(sf) # manipulate shapefiles 
library(rgeoboundaries) # international boundaries 
library(ggrepel)  # install.packages("ggrepel")
library(ggthemes) # install.packages("ggthemes")
library(crsuggest) # install.packages("crsuggest")
library(ggarchery) # install.packages("ggarchery") for easy arrows

# Global Options

theme_set(theme_clean())

# read data in - see README for link

anes_1948_2020 <- 
  read_csv("data/anes_timeseries_cdf_csv_20220916.csv")

# tidying, getting years to 2000 - 2020 and pre-election vs post-election

anes_2000_2020 <- 
  anes_1948_2020 |>
  select(
    # Year
    VCF0004, 
    
    # Weight
    VCF9999, # post-election weight
    
    # Completion
    VCF0013, # post-election
    VCF0014, # pre-election
    VCF0748, # voted on election day or before
    
    # Dependent Variable
    VCF0302, # initial party identification response
    VCF0303, # party identification response
    VCF0705, # vote for president - major parties and other
    
    # Demographic
    VCF0101, # age
    VCF0104, # gender
    VCF0105b, # race-ethnicity
    VCF0110, # education
    VCF0147, # marital status
    VCF0111, # urbanism (not available after 2000, drop or generated by spatial information)
    
    # Economic Condition
    VCF0114, # family-income
    VCF0116, # work status
    VCF0151, # occupation group (not available)
    VCF0146, # home ownership
    VCF9224, # stock market investment
    
    # Ideology
    VCF0128, # religion
    VCF0201:VCF0291, # attitude to different interest groups
    matches("^VCF08"), # policy preference
    matches("^VCF92"), # political opinion & VCF9277-82 personal info
    
    # Family Information
    VCF0306, # party identification of R's father
    VCF0307, # party identification of R's mother
    VCF0308, # political interest of R's father
    VCF0309, # political interest of R's mother
    VCF0138, # number of children in family
  
    
    # Spatial Information
    VCF0900, # congressional district of residence
    VCF0900b, # state and congressional district - fips
    VCF0900c, # state and congressional district - postal abbrev and cd
    VCF0901a, # state code - fips
    VCF0901b, # state postal abbrev
  ) |>
  filter(VCF0004 %in% c(2000:2020), 
         VCF0014 == 1 & VCF0013 == 1, 
         is.na(VCF9999) == FALSE,
         VCF0748 %in% c(1,5), 
         VCF0705 > 0, # 0 - did not vote, declined to answer
         ) |> 
  # transform missing value to NAs
  mutate(VCF0101 = ifelse(VCF0101 == 0, NA, VCF0101)) |>
  mutate(VCF0104 = ifelse(VCF0104 == 0, NA, VCF0104)) |>
  mutate(VCF0105b = ifelse(VCF0105b == 9, NA, VCF0105b)) |>
  mutate(VCF0110 = ifelse(VCF0110 == 0, NA, VCF0110)) |>
  mutate(VCF0147 = ifelse(VCF0147 %in% c(8,9), NA, VCF0147)) |>
  mutate(VCF0114 = ifelse(VCF0114 == 0, NA, VCF0114)) |>
  mutate(VCF0116 = ifelse(VCF0116 == 9, NA, VCF0116)) |>
  mutate(VCF0146 = ifelse(VCF0146 == 9, NA, VCF0146)) |> # high percentage of missing data
  mutate(VCF9224 = ifelse(VCF9224 %in% c(-8,-9), NA, VCF9224)) |>
  mutate(VCF0128 = ifelse(VCF0128 == 0, NA, VCF0128)) |>
  mutate(across(VCF0201:VCF0253, ~ ifelse(. %in% c(98, 99), NA, .))) |>
  mutate(VCF0290 = ifelse(VCF0290 %in% c(998, 999), NA, VCF0290)) |>
  mutate(VCF0291 = ifelse(VCF0291 %in% c(998, 999), NA, VCF0291)) |>
  mutate(VCF0801 = ifelse(VCF0801 %in% c(98, 99), NA, VCF0801)) |>
  mutate(VCF0803 = ifelse(VCF0803 %in% c(0, 9), NA, VCF0803)) |>
  mutate(VCF0804 = ifelse(VCF0804 %in% c(0, 9), NA, VCF0804)) |>
  mutate(across(VCF0806:VCF0823, ~ ifelse(. %in% c(0, 9), NA, .))) |>
  mutate(VCF0825 = ifelse(VCF0825 %in% c(0, 9), NA, VCF0825)) |>
  mutate(VCF0826 = ifelse(VCF0825 %in% c(0, 9), NA, VCF0825)) |>
  mutate(VCF0828 = ifelse(VCF0828 == 0, NA, VCF0828)) |>
  mutate(VCF0829 = ifelse(VCF0829 == 0, NA, VCF0829)) |>
  mutate(VCF0830 = ifelse(VCF0830 == 0, NA, VCF0830)) |>
  mutate(VCF0838 = ifelse(VCF0838 == 0, NA, VCF0838)) |>
  mutate(VCF0839 = ifelse(VCF0839 == 0, NA, VCF0839)) |>
  mutate(VCF0843 = ifelse(VCF0843 == 0, NA, VCF0843)) |>
  mutate(VCF0846 = ifelse(VCF0846 %in% c(0,8), NA, VCF0846)) |>
  mutate(VCF0849 = ifelse(VCF0849 %in% c(0,9), NA, VCF0849)) |>
  mutate(VCF0850 = ifelse(VCF0850 %in% c(0,9), NA, VCF0850)) |>
  mutate(VCF0852 = ifelse(VCF0852 == 9, NA, VCF0852)) |>
  mutate(VCF0853 = ifelse(VCF0853 == 9, NA, VCF0853)) |>
  mutate(VCF0867 = ifelse(VCF0867 == 9, NA, VCF0867)) |>
  mutate(VCF0870 = ifelse(VCF0870 %in% c(0,8), NA, VCF0870)) |>
  mutate(VCF0871 = ifelse(VCF0871 %in% c(0,8,9), NA, VCF0871)) |>
  mutate(VCF0872 = ifelse(VCF0872 %in% c(8,9), NA, VCF0872)) |>
  mutate(VCF0876 = ifelse(VCF0876 == 9, NA, VCF0876)) |>
  mutate(VCF0876a = ifelse(VCF0876a == 9, NA, VCF0876a)) |>
  mutate(VCF0878 = ifelse(VCF0878 == 9, NA, VCF0878)) |>
  mutate(VCF0879a = ifelse(VCF0879a == 9, NA, VCF0879a)) |>
  mutate(VCF0880 = ifelse(VCF0880 %in% c(0,9), NA, VCF0880)) |>
  mutate(VCF0880a = ifelse(VCF0880a %in% c(0,9), NA, VCF0880a)) |>
  mutate(VCF0881 = ifelse(VCF0881 %in% c(0,9), NA, VCF0881)) |>
  mutate(VCF0886 = ifelse(VCF0886 == 9, NA, VCF0886)) |>
  mutate(VCF0888 = ifelse(VCF0888 == 9, NA, VCF0888)) |>
  mutate(VCF0890 = ifelse(VCF0890 == 9, NA, VCF0890)) |>
  mutate(VCF0894 = ifelse(VCF0894 == 9, NA, VCF0894)) |>
  select(-where(~sum(is.na(.)) > 1000)) |>
  select(-c(VCF0004,VCF0901b, VCF0013,VCF0014))

# Note: set up outcome variable as factor for classification - maybe make into a step?

# Note: step_num2factor interferes with LASSO since testing data isn't factorized 

# anes_2000_2020$VCF0705 <- as.character(anes_2000_2020$VCF0705)

# anes_2000_2020$VCF0705 <- fct(anes_2000_2020$VCF0705)

```


## Model 1: Random Forests model with resampling

```{r}
#|label: random forests model


anes_2000_2020 <- 
  anes_2000_2020 |>
  mutate(VCF9999 = importance_weights(VCF9999)) # create recipe case weight 

anes_split <- initial_split(anes_2000_2020)

anes_train <- training(x = anes_split)

anes_test <- testing(x = anes_split)

anes_folds <- vfold_cv(data = anes_train, v = 5)

anes_recipe <- 
  recipe(formula = VCF0705~ ., data = anes_train) |>
  step_num2factor(VCF0705, levels = c("1","2","3")) |>
  step_impute_bag(all_predictors(), trees = 5)
  
anes_rf_mod <- 
    rand_forest(
      trees = 500,
      mtry = 5,
      min_n = 4
    ) |>
  set_mode(mode = "classification") |>
  set_engine(
    engine = "ranger", 
    importance = "impurity",
    num.threads = 4
  )

anes_rf_wf <- 
  workflow() |>
  add_recipe(anes_recipe) |>
  add_model(anes_rf_mod) |>
  add_case_weights(VCF9999)



anes_rf_resamples <- 
  anes_rf_wf |>
  fit_resamples(resamples = anes_folds)

collect_metrics(anes_rf_resamples)

anes_rf_final <- 
  anes_rf_wf |>
  last_fit(anes_split)

anes_vip_plot <- 
  anes_rf_final |>
  extract_fit_parsnip() |>
  vip(num_features = 20)

anes_vip_plot + ggtitle("Variable Importance in Determining Voter Preference") 

ggsave(filename = "anes_rf_vip_plot.png")

```

## Model 2: Bagged Trees with hyperparameter tuning 

```{r}
#|label: bagging with hp tuning

anes_bagged_trees_model <- 
  bag_tree(cost_complexity = tune(), tree_depth = tune()) |>
  set_engine("rpart") |> 
  set_mode("classification")

anes_bagged_folds <- vfold_cv(data = anes_train, v = 5)

anes_bagged_grid <- grid_regular(cost_complexity(),tree_depth(), levels = 3)

anes_bagged_trees_wf <-
  workflow() |>
  add_recipe(anes_recipe) |>
  add_model(spec = anes_bagged_trees_model) |>
  add_case_weights(VCF9999)

anes_bagged_trees_resamples <- 
  anes_bagged_trees_wf |>
  tune_grid(
    resamples = anes_bagged_folds,
    grid = anes_bagged_grid
  )

collect_metrics(anes_bagged_trees_resamples) 

anes_bagged_trees_resamples |>
  show_best(metric = "roc_auc")

best_bagged_value <- 
  anes_bagged_trees_resamples |>
  select_best(metric = "roc_auc")

anes_bagged_best_wf <- 
  anes_bagged_trees_wf |>
  finalize_workflow(best_bagged_value)
```

Now, we run our bagged tree model with the best parameters given by hyperparameter tuning: 

```{r}
#|label: applying hyperparametering tuning to final bagged tree model


# issue: since predict() doesn't have VCF0705 

anes_bagged_finalized_fit <- 
  anes_bagged_best_wf |>
  last_fit(anes_split)

anes_bagged_pred <- 
  anes_bagged_finalized_fit |>
  predict(new_data = anes_test)

anes_bagged_finalized_fit |>
  collect_metrics()

anes_prediction <-
  predict(anes_bagged_finalized_fit, new_data = anes_test) |>
  bind_cols(
    predict(anes_bagged_finalized_fit, new_data = anes_test, type = "prob"),
    anes_test |>
    select(VCF0705)
  )


```


## Model 3: Na√Øve Classification Decision Tree 

```{r}
#|label: decision tree 

# Note: maybe use tuning here? Model doesn't say a lot here (but just enough)

anes_cart_model <- 
  decision_tree() |>
  set_engine(engine = "rpart") |>
  set_mode(mode = "classification")

anes_cart_wf <- 
  workflow() |>
  add_recipe(anes_recipe) |>
  add_model(anes_cart_model) |>
  add_case_weights(VCF9999)
  
anes_cart_fit <- 
  anes_cart_wf |>
  fit(data = anes_train)

# create a tree
rpart.plot::rpart.plot(x = anes_cart_fit$fit$fit$fit)

```


## Model 4: Elastic Net Regression Logistic Classification

Open issue: having trouble making Elastic graph even with textbook, need to talk to Mia or Aaron 

```{r}
#|label: ENR Classification using multinomial regression


# Notes: using glmnet engine and low penalty with 50/50 ridge/LASSO

anes_elastic_function <- 
  function(penalty){
    
anes_elastic_recipe <- 
  anes_recipe |>
  step_normalize(all_numeric_predictors())
  
anes_elastic_model <- 
  multinom_reg(penalty = penalty, mixture = .5) |>
  set_mode(mode = "classification") |>
  set_engine(engine = "glmnet")  

anes_elastic_wf <- 
  workflow() |>
  add_recipe(recipe = anes_lasso_recipe) |>
  add_model(spec = anes_lasso_model) |> 
  add_case_weights(VCF9999)

  # see tidy coef fits 

anes_elastic_wf |>
  fit(data = anes_train) |>
  extract_fit_parsnip() |>
  tidy() |>
  mutate(penalty = penalty)


}

# iterate over possible penalties for graph

candidate_penalties <- seq(0,2,1)

elastic_penalties <- 
  map_dfr(.x = candidate_penalties,
          .f = anes_elastic_function(.x)
          )

ggplot() +
  geom_line(data = filter(elastic_penalties, term != "(Intercept)"),
            aes(x = penalty, y = estimate, group = term)) + 
  geom_point(data = filter(elastic_penalties, term != "(Intercept)"),
             aes(x = penalty, y = estimate)) +
  geom_label_repel(data = filter(elastic_penalties, 
                                 term != "(Intercept)", 
                                 estimate != 0),
                   aes(x = penalty, y = estimate, label = term)
                   )



anes_lasso_predictions <-  
  bind_cols(
    anes_test,
    predict(lasso_glm_fit, new_data = anes_test),
    predict(lasso_glm_fit, new_data = anes_test, type = "prob")
    )

select(anes_lasso_predictions, VCF0705, starts_with(".pred"))

```
Getting this error trying to test predictive models for all models:

Error in `step_num2factor()`:
! The following required column is missing from `new_data` in step
  'num2factor_ug3md': VCF0705.


### LASSO Regression Model 

```{r}
#| eval: false
  

# old reference code - not to be evaluated 

anes_lassolog_model <- 
  logistic_reg(penalty = 2, mixture = 1) |>
  set_mode(mode = "classification") |>
  set_engine(engine = "glm")  

anes_lasso_wf <- 
  workflow() |>
  add_recipe(recipe = anes_lasso_recipe) |>
  add_model(spec = anes_lassolog_model)

anes_lasso_wf |>
  fit(data = anes_train) |>
  extract_fit_parsnip() |>
  tidy()

```

## Using knowledge from above to predict Korean voter sentiment 

```{r}
#| label: setup for using korean survey data

# Note: Follow instructions in icpsr R file

load("data/38577-0001-Data.rda")

icpsr_korean_survey <- da38577.0001


library(prettyR)

lbls <- sort(levels(da99999.0001$MYVAR))
lbls <- (sub("^\\([0-9]+\\) +(.+$)", "\\1", lbls))
da99999.0001$MYVAR <- as.numeric(sub("^\\(0*([0-9]+)\\).+$", "\\1", da99999.0001$MYVAR))
da99999.0001$MYVAR <- add.value.labels(da99999.0001$MYVAR, lbls)

```
